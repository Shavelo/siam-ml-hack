{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бейзлайн модель для обнаружения паттернов КВД и КПД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфигурация\n",
    "DATA_PATH = \"../data/raw/\"\n",
    "WINDOW_SIZE = 60   # точек в окне\n",
    "STEP = 10         # шаг окна\n",
    "MIN_DURATION = 4  # часов для recovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Загрузка данных и разметки\n",
    "# ---------------------------------------------------\n",
    "def load_data(file_id, is_test=False):\n",
    "    \"\"\"Загрузка временного ряда по ID файла\"\"\"\n",
    "    # Определяем путь в зависимости от типа данных\n",
    "    folder = \"test\" if is_test else \"Task 21\"\n",
    "    path = os.path.join(DATA_PATH, folder, file_id)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Файл {file_id} не найден в папке {folder}.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=['time', 'pressure'])\n",
    "    return df.sort_values('time').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Предобработка данных\n",
    "# ---------------------------------------------------\n",
    "def preprocess_series(df):\n",
    "    \"\"\"Сглаживание и нормализация\"\"\"\n",
    "    df['pressure'] = df['pressure'].rolling(5, center=True, min_periods=1).mean()\n",
    "    df['pressure'] = (df['pressure'] - df['pressure'].min()) / \\\n",
    "                    (df['pressure'].max() - df['pressure'].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Генерация признаков для окон\n",
    "# ---------------------------------------------------\n",
    "from scipy.fftpack import fft\n",
    "from scipy.integrate import simpson  # Используем simpson вместо simps\n",
    "\n",
    "def get_window_features(window):\n",
    "    \"\"\"Извлечение признаков из окна\"\"\"\n",
    "    time = window['time'].values\n",
    "    pressure = window['pressure'].values\n",
    "    \n",
    "    # Базовые признаки\n",
    "    features = {\n",
    "        'slope': np.polyfit(time, pressure, 1)[0],\n",
    "        'mean': np.mean(pressure),\n",
    "        'std': np.std(pressure),\n",
    "        'max_diff': np.max(pressure) - np.min(pressure),\n",
    "        'duration': time[-1] - time[0],\n",
    "        'starts_with_max': int(pressure[0] == np.max(pressure))  # Новый признак\n",
    "    }\n",
    "    \n",
    "    # Частотные признаки (преобразование Фурье)\n",
    "    fft_values = np.abs(fft(pressure))\n",
    "    features['fft_peak'] = np.max(fft_values)\n",
    "    features['fft_mean'] = np.mean(fft_values)\n",
    "    \n",
    "    # Производные\n",
    "    first_derivative = np.gradient(pressure, time)\n",
    "    second_derivative = np.gradient(first_derivative, time)\n",
    "    features['first_derivative_max'] = np.max(first_derivative)\n",
    "    features['second_derivative_max'] = np.max(second_derivative)\n",
    "    \n",
    "    # Интеграл (площадь под кривой)\n",
    "    features['integral'] = simpson(pressure, time)\n",
    "    \n",
    "    # Автокорреляция (лаги 1, 2, 3)\n",
    "    for lag in [1, 2, 3]:\n",
    "        features[f'autocorr_lag_{lag}'] = np.corrcoef(pressure[:-lag], pressure[lag:])[0, 1]\n",
    "    \n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Подготовка датасета для обучения\n",
    "# ---------------------------------------------------\n",
    "def create_dataset(file_ids, gt):\n",
    "    \"\"\"Создание размеченного датасета с проверкой существования файлов\"\"\"\n",
    "    features = []\n",
    "    targets = []\n",
    "    \n",
    "    # Проверка наличия файлов в Task 21\n",
    "    valid_files = []\n",
    "    for file_id in file_ids:\n",
    "        path = os.path.join(DATA_PATH, \"Task 21\", file_id)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Файл {file_id} из разметки отсутствует в Task 21 и будет пропущен.\")\n",
    "        else:\n",
    "            valid_files.append(file_id)\n",
    "    \n",
    "    # Обработка только валидных файлов\n",
    "    for file_id in tqdm(valid_files):\n",
    "        df = load_data(file_id, is_test=False)\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        df = preprocess_series(df)\n",
    "        \n",
    "        # Получение разметки для файла\n",
    "        file_gt = gt[gt['file'] == file_id].iloc[0]\n",
    "        recovery = eval(file_gt['recovery'])\n",
    "        drop = eval(file_gt['drop'])\n",
    "        \n",
    "        # Скользящее окно\n",
    "        for i in range(0, len(df)-WINDOW_SIZE, STEP):\n",
    "            window = df.iloc[i:i+WINDOW_SIZE]\n",
    "            window_features = get_window_features(window)\n",
    "            \n",
    "            # Проверка попадания в разметку\n",
    "            target = 0\n",
    "            start = window['time'].iloc[0]\n",
    "            end = window['time'].iloc[-1]\n",
    "            \n",
    "            for interval in recovery:\n",
    "                if interval[0] <= start and end <= interval[1]:\n",
    "                    target = 1  # recovery\n",
    "            for interval in drop:\n",
    "                if interval[0] <= start and end <= interval[1]:\n",
    "                    target = 2  # drop\n",
    "                    \n",
    "            features.append(window_features)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return pd.DataFrame(features), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датасета\n",
    "gt = pd.read_csv(os.path.join(DATA_PATH, \"ground_truth.csv\"))\n",
    "train_files = gt['file']\n",
    "X, y = create_dataset(train_files, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 5. Обучение модели с кросс-валидацией\n",
    "# ---------------------------------------------------\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Настройки модели с лучшими параметрами\n",
    "model_params = {\n",
    "    'iterations': 1500,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'class_weights': [1, 20, 20],  # Увеличенные веса для recovery и drop\n",
    "    'task_type': 'CPU',  # Для GPU: 'GPU'\n",
    "    'verbose': 200\n",
    "}\n",
    "\n",
    "# Кросс-валидация\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**model_params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "    \n",
    "    # Прогноз и метрики\n",
    "    y_pred = model.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_pred, target_names=['background', 'recovery', 'drop']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод метрик\n",
    "for i, report in enumerate(results):\n",
    "    print(f\"Fold {i+1}:\\n{report}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение финальной модели на всех данных\n",
    "model = CatBoostClassifier(**model_params)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Сохранение финальной модели\n",
    "#model.save_model('model+.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Предсказание и постобработка\n",
    "# ---------------------------------------------------\n",
    "def predict_intervals(model, file_id, is_test=False):\n",
    "    \"\"\"Предсказание интервалов для одного файла\"\"\"\n",
    "    df = load_data(file_id, is_test=is_test)\n",
    "    if df is None:\n",
    "        return {'recovery': [], 'drop': []}\n",
    "    \n",
    "    df = preprocess_series(df)\n",
    "    \n",
    "    # Прогноз для всего ряда\n",
    "    window_preds = []\n",
    "    for i in range(0, len(df)-WINDOW_SIZE, STEP):\n",
    "        window = df.iloc[i:i+WINDOW_SIZE]\n",
    "        features = get_window_features(window)\n",
    "        pred = model.predict(pd.DataFrame([features]))[0]\n",
    "        proba = model.predict_proba(pd.DataFrame([features]))[0][pred]\n",
    "        window_preds.append( (window['time'].iloc[0], window['time'].iloc[-1], pred, proba) )\n",
    "    \n",
    "    # Фильтрация по вероятности\n",
    "    filtered = [ (start, end, label) for start, end, label, proba in window_preds if proba > 0.7 ]\n",
    "    \n",
    "    # Объединение интервалов\n",
    "    merged = []\n",
    "    current = None\n",
    "    for start, end, label in filtered:\n",
    "        if label == 0:\n",
    "            continue\n",
    "        if current and current[2] == label and current[1] >= start - 2:  # Зазор 2 часа\n",
    "            current = (current[0], end, label)\n",
    "        else:\n",
    "            if current:\n",
    "                merged.append(current)\n",
    "            current = (start, end, label)\n",
    "    if current:\n",
    "        merged.append(current)\n",
    "    \n",
    "    \n",
    "    # Оставляем только 1 лучший drop\n",
    "    drop_intervals = [i for i in merged if i[2] == 2]\n",
    "    if drop_intervals:\n",
    "        drop_intervals = [max(drop_intervals, key=lambda x: x[1] - x[0])]  # Только 1 самый длинный\n",
    "\n",
    "    # Перезапись отфильтрованных drop в merged\n",
    "    merged = [i for i in merged if i[2] != 2] + drop_intervals\n",
    "    \n",
    "    # Оставляем только 1 самый длинный recovery\n",
    "    recovery_intervals = [i for i in merged if i[2] == 1]\n",
    "    if recovery_intervals:\n",
    "        recovery_intervals = [max(recovery_intervals, key=lambda x: x[1] - x[0])]  # Только 1 самый длинный\n",
    "\n",
    "    # Перезапись отфильтрованных recovery в merged\n",
    "    merged = [i for i in merged if i[2] != 1] + recovery_intervals\n",
    "    \n",
    "\n",
    "    \n",
    "    # Фильтрация по длительности и физическим ограничениям\n",
    "    result = {'recovery': [], 'drop': []}\n",
    "    for start, end, label in merged:\n",
    "        duration = end - start\n",
    "        if label == 1 and duration >= 6:  # Минимум 6 часов для recovery\n",
    "            result['recovery'].append([start, end])\n",
    "        elif label == 2 and duration >= 10:  # Минимум 10 часов для drop\n",
    "            # Проверка, что drop начинается с максимума давления\n",
    "            window = df[(df['time'] >= start) & (df['time'] <= end)]\n",
    "            if window['pressure'].iloc[0] == window['pressure'].max():\n",
    "                result['drop'].append([start, end])\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Визуализация результатов\n",
    "# ---------------------------------------------------\n",
    "def plot_comparison(file_id, pred_intervals, gt_entry):\n",
    "    \"\"\"\n",
    "    Визуализация предсказаний и истинных интервалов.\n",
    "    \n",
    "    :param file_id: ID файла (строка).\n",
    "    :param pred_intervals: Предсказанные интервалы (словарь с ключами 'recovery' и 'drop').\n",
    "    :param gt_entry: Строка из ground_truth.csv с истинными интервалами.\n",
    "    \"\"\"\n",
    "    # Загрузка данных\n",
    "    df = load_data(file_id)\n",
    "    if df is None:\n",
    "        print(f\"Файл {file_id} не найден.\")\n",
    "        return\n",
    "    \n",
    "    # Создание двух графиков\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "    \n",
    "    # График 1: Предсказанные интервалы\n",
    "    ax1.plot(df['time'], df['pressure'], label='Давление', color='blue')\n",
    "    \n",
    "    # Предсказанные recovery\n",
    "    for interval in pred_intervals['recovery']:\n",
    "        start, end = interval\n",
    "        ax1.axvspan(start, end, alpha=0.2, color='green')\n",
    "    \n",
    "    # Предсказанные drop\n",
    "    for interval in pred_intervals['drop']:\n",
    "        start, end = interval\n",
    "        ax1.axvspan(start, end, alpha=0.2, color='red')\n",
    "    \n",
    "    ax1.set_title(f\"Предсказанные интервалы для {file_id}\")\n",
    "    ax1.set_ylabel(\"Нормализованное давление\")\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # График 2: Истинные интервалы\n",
    "    ax2.plot(df['time'], df['pressure'], label='Давление', color='blue')\n",
    "    \n",
    "    # Истинные recovery\n",
    "    for interval in eval(gt_entry['recovery']):\n",
    "        ax2.axvspan(interval[0], interval[1], alpha=0.2, color='green', label='True Recovery')\n",
    "    \n",
    "    # Истинные drop\n",
    "    for interval in eval(gt_entry['drop']):\n",
    "        ax2.axvspan(interval[0], interval[1], alpha=0.2, color='red', label='True Drop')\n",
    "    \n",
    "    ax2.set_title(f\"Истинные интервалы для {file_id}\")\n",
    "    ax2.set_xlabel(\"Время (часы)\")\n",
    "    ax2.set_ylabel(\"Нормализованное давление\")\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Удаление дубликатов в легенде\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax2.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования\n",
    "test_file = gt['file'].iloc[23]\n",
    "\n",
    "predicted = predict_intervals(model, test_file)\n",
    "gt_entry = gt[gt['file'] == test_file].iloc[0]\n",
    "\n",
    "plot_comparison(test_file, predicted, gt_entry)\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 8. Генерация сабмита\n",
    "# ---------------------------------------------------\n",
    "def generate_submission(model, test_files, output_path):\n",
    "    \"\"\"Генерация файла для отправки на платформу\"\"\"\n",
    "    submission = []\n",
    "    \n",
    "    for file_path in tqdm(test_files):\n",
    "        # Извлекаем только имя файла без пути\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Предсказание с флагом is_test=True\n",
    "        preds = predict_intervals(model, file_name, is_test=True)\n",
    "        \n",
    "        submission.append({\n",
    "            'file': file_name,\n",
    "            'recovery': str(preds['recovery']).replace(' ', ''),\n",
    "            'drop': str(preds['drop']).replace(' ', '')\n",
    "        })\n",
    "    \n",
    "    # Сохранение в CSV\n",
    "    pd.DataFrame(submission).to_csv(output_path, index=False)\n",
    "    print(f\"Сабмит сохранен в {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к тестовым данным\n",
    "TEST_PATH = os.path.join(DATA_PATH, \"test\")\n",
    "\n",
    "# Получаем список всех файлов в тестовой папке\n",
    "test_files = [os.path.join(TEST_PATH, f) for f in os.listdir(TEST_PATH) \n",
    "              if os.path.isfile(os.path.join(TEST_PATH, f))]\n",
    "\n",
    "# Генерация сабмита\n",
    "generate_submission(\n",
    "    model=model,\n",
    "    test_files=test_files,\n",
    "    output_path=\"../data/submissions/submission_model+_max1drop_max1recovery_Window60.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_drops(submission_path, output_path):\n",
    "    \"\"\"\n",
    "    Удаляет все интервалы drop из сабмита и сохраняет результат в новый файл.\n",
    "    \n",
    "    :param submission_path: Путь к исходному файлу сабмита.\n",
    "    :param output_path: Путь для сохранения нового файла без drop.\n",
    "    \"\"\"\n",
    "    # Загрузка сабмита\n",
    "    submission = pd.read_csv(submission_path)\n",
    "    \n",
    "    # Удаление всех интервалов drop\n",
    "    submission['drop'] = submission['drop'].apply(lambda x: '[]')\n",
    "    \n",
    "    # Сохранение результата\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"Сабмит без drop сохранен в {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_drops(\"../data/submissions/submission_model+_max2drop.csv\", \"../data/submissions/submission_model+_without_drop.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
